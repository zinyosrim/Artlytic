{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import errno\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from abc import ABCMeta\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from dateutil import relativedelta\n",
    "from lxml import html\n",
    "from pathlib import Path\n",
    "from random import randint\n",
    "from time import sleep\n",
    "import wikiart\n",
    "import auctionhouse \n",
    "#from christies import Christies\n",
    "#from sothebys import Sothebys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Artlytic(object):\n",
    "    '''Main class to load data and analyse'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        # data paths\n",
    "        self.data_path         = 'data/'\n",
    "        \n",
    "        self.search_URLs_path  = self.data_path + 'search_urls.json'\n",
    "        self.lot_URLs_path     = self.data_path + 'lot_urls.json'\n",
    "        self.lots_path         = self.data_path + 'lots.json'\n",
    "        self.auctions_path     = self.data_path + 'auctions.json'\n",
    "        self.artists_path      = self.data_path + 'wikiart/wikiart_artists_19th_century.json'\n",
    "        # data lists\n",
    "        self.search_URLs = []\n",
    "        self.auctions    = []\n",
    "        self.lot_URLs    = []\n",
    "        self.artists     = []\n",
    "        self.lots        = []\n",
    "        # create objects\n",
    "        christies = Christies()\n",
    "        sothebys  = Sothebys()\n",
    "        # populate lists if data exists\n",
    "        self.load_existing_data()\n",
    "\n",
    "\n",
    "    def create_datafiles_upon_existence_check(self):\n",
    "        '''Checks if there are any datafiles. If not, creates empty data files'''\n",
    "        \n",
    "        # for each datafile check if file exists. If not create en empty json file\n",
    "        for file in [self.search_URLs_path,\n",
    "                     self.auctions_path,\n",
    "                     self.lot_URLs_path, \n",
    "                     self.lots_path,\n",
    "                     self.artists_path]:\n",
    "            flags = os.O_CREAT | os.O_EXCL | os.O_WRONLY\n",
    "            try:\n",
    "                file_handle = os.open(file, flags)\n",
    "            except OSError as e:\n",
    "                if e.errno == errno.EEXIST:  # Failed as the file already exists.\n",
    "                    pass\n",
    "                else:  # Something unexpected went wrong so reraise the exception.\n",
    "                    raise\n",
    "            else:  # No exception, so the file must have been created successfully.\n",
    "                with os.fdopen(file_handle, 'a') as file_obj:\n",
    "                    file_obj.write(\"[]\")\n",
    "\n",
    "                    \n",
    "    def load_existing_data(self):\n",
    "        '''Populates the lists for lots, search URLs, artists with existing data. \n",
    "        If there's no data empty files are created.'''\n",
    "        \n",
    "        # check existince of data files\n",
    "        self.create_datafiles_upon_existence_check()\n",
    "        \n",
    "        # load the data\n",
    "        if os.stat(self.search_URLs_path).st_size > 0:\n",
    "            with open(self.search_URLs_path) as fp:\n",
    "                self.search_URLs = json.load(fp)\n",
    "            print('loaded', self.search_URLs_path, 'to search_URLs with', len(self.search_URLs), 'entries')\n",
    "        if os.stat(self.lot_URLs_path).st_size > 0:\n",
    "            with open(self.lot_URLs_path) as fp:\n",
    "                self.lot_URLs = json.load(fp)\n",
    "            print('loaded', self.lot_URLs_path, 'to self.lot_URLs_path', len(self.lot_URLs), 'entries')\n",
    "        if os.stat(self.lots_path).st_size > 0:\n",
    "            with open(self.lots_path) as fp:\n",
    "                self.lots = json.load(fp)\n",
    "            print('loaded', self.lots_path, 'to self.lots_path with', len(self.lots), 'entries')\n",
    "        if os.stat(self.artists_path).st_size > 0:\n",
    "            with open(self.artists_path) as fp:\n",
    "                self.artists = json.load(fp)\n",
    "            print('loaded', self.artists_path, 'to self.artists with', len(self.artists), 'entries')\n",
    "\n",
    "            \n",
    "    def update_lotlist_from_search_url(self):\n",
    "        '''Iterate the search_URLs, unless already scraped, fetch the content, parse the URLs,\n",
    "        add them to the lot_URLs. Save the new list as file to self.lot_URLs_path.'''  \n",
    "        \n",
    "        # load artlytic.search_URLs_path to artlytic.search_URLs\n",
    "        try:\n",
    "            with open(self.search_URLs_path) as fp:\n",
    "                self.search_URLs = json.load(fp)\n",
    "            print('loaded', self.search_URLs_path, 'to search_URLs with', len(self.search_URLs), 'entries')\n",
    "        except IOError:\n",
    "            print('*** Could not read file:', self.search_URLs_path)\n",
    "            return\n",
    "        \n",
    "        # load artlytic.lot_URLs_path to artlytic.lot_URLs\n",
    "        try:\n",
    "            with open(self.lot_URLs_path) as fp:\n",
    "                self.lot_URLs = json.load(fp)\n",
    "            print('loaded', self.lot_URLs_path, 'to lot_URLs with', len(self.lot_URLs), 'entries')\n",
    "        except IOError:\n",
    "            print('*** Could not read file:', self.lot_URLs_path)\n",
    "            return\n",
    "        \n",
    "        print('Beginning to create lot_URLs entries from search_URLs list')\n",
    "        print('Currently there are', len(self.lot_URLs),'entries in artlytic.lot_URLs')\n",
    "        # a number of counters for outputing the status\n",
    "        counter_all_newly_added_entries = 0\n",
    "        skipped_search_url_entries_counter = 0\n",
    "        processed_search_url_entry_number = 0\n",
    "        search_URLs_list_size = len(self.search_URLs)\n",
    "        \n",
    "        # process search_URLs entries one by one. Scrabe the search_URL, parse lot_URLs, add them to artlytic.lot_URLs\n",
    "        for search_url_entry in self.search_URLs:\n",
    "            if search_url_entry['scraped']:  \n",
    "                # search_URLs entry was processed already. Do nothing\n",
    "                skipped_search_url_entries_counter += 1\n",
    "                processed_search_url_entry_number += 1\n",
    "                pass\n",
    "            else:                                   \n",
    "                # process search URL\n",
    "                counter = 0\n",
    "                url_list = []\n",
    "                \n",
    "                # parse the page and return the lot URLs \n",
    "                auction_house = search_url_entry['auction_house']\n",
    "                if auction_house == \"Christie's\" : \n",
    "                    url_list = christies.parse_search_results_page(search_url_entry['url'])\n",
    "                elif auction_house == \"Sotheby's\" : \n",
    "                    url_list = sothebys.parse_search_results_page(search_url_entry['url'])\n",
    "                elif auction_house == \"Phillips\" : \n",
    "                    url_list = phillips.parse_search_results_page(search_url_entry['url'])\n",
    "                else: print('Auction house not known:', auction_house)\n",
    "                    \n",
    "                # iterate the lot urls and add them to lot_URLs \n",
    "                for url in url_list: \n",
    "                    # add new entry to lot_URLs\n",
    "                    new_lot_url_entry = {}\n",
    "                    new_lot_url_entry['auction_house'] = search_url_entry['auction_house']\n",
    "                    new_lot_url_entry['url'] = url\n",
    "                    new_lot_url_entry['scraped'] = False\n",
    "                    new_lot_url_entry['parsed'] = False\n",
    "                    new_lot_url_entry['timestamp'] = datetime.now().isoformat().replace(':', '-').replace('.', '-')\n",
    "                    self.lot_URLs.append(new_lot_url_entry)\n",
    "                    counter = counter + 1\n",
    "                processed_search_url_entry_number += 1\n",
    "                print(processed_search_url_entry_number,'/', search_URLs_list_size, \\\n",
    "                      ': Added', counter,'entries for',\\\n",
    "                      search_url_entry['auction_house'], search_url_entry['artist_name'])\n",
    "                \n",
    "                # update the search_URLs entry, so it doesn't get scraped again next time\n",
    "                search_url_entry['scraped'] = True\n",
    "                search_url_entry['timestamp'] = datetime.now().isoformat().replace(':', '-').replace('.', '-')  \n",
    "                counter_all_newly_added_entries += counter\n",
    "                sleep(randint(0,2))\n",
    "                \n",
    "        print('Updated lot_URLs with',counter_all_newly_added_entries,'new artworks. New size:', len(self.lot_URLs)) \n",
    "        print('Skipped', skipped_search_url_entries_counter, 'already processed search URLs' )\n",
    "        \n",
    "        # write lot_URLs list to file\n",
    "        try:\n",
    "            with open(self.lot_URLs_path, 'w') as fp:\n",
    "                json.dump(self.lot_URLs, fp, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "            print('Updated', self.lot_URLs_path, '. File contains now', len(self.lot_URLs), 'entries')\n",
    "        except IOError: \n",
    "            print('*** something went wrong. there was no', self.lot_URLs_path, '. Check why file is not there!')\n",
    "            return\n",
    "        \n",
    "        # write search_URLs list to file\n",
    "        try:\n",
    "            with open(self.search_URLs_path, 'w') as fp:\n",
    "                json.dump(self.search_URLs, fp, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "            print('Updated scraped status in', self.search_URLs_path)\n",
    "        except IOError: \n",
    "            print('*** something went wrong. there was no', self.search_URLs_path, '. Check why file is not there!')\n",
    "            return\n",
    "\n",
    "        \n",
    "    def eliminate_duplicates_from_lot_URLs(self):\n",
    "        '''Eliminate duplicates in the self.lot_URLs list and output the list again to self.lot_URLs_path.\n",
    "        Because we try to gather a max. of lots of an artist sometimes we use more than one search URL. \n",
    "        As a result, the chances are high, that a number of items are duplicates. To eliminate these, \n",
    "        we import the lot URLs into a dataframe, eliminate the duplicates and update the URL list thereafter.'''\n",
    "        \n",
    "        # load lot_URLs into a Pandas DataFrame\n",
    "        with open(self.lot_URLs_path) as json_file:\n",
    "            self.lot_URLs = json.load(json_file)\n",
    "\n",
    "        lot_URLs_dataframe = pd.DataFrame(self.lot_URLs)\n",
    "        old_size = len(lot_URLs_dataframe)\n",
    "        print('Size of lot_URLs before eliminating duplicates:', old_size)\n",
    "\n",
    "        # remove duplicates\n",
    "        lot_URLs_dataframe.drop_duplicates(subset='url', keep='first', inplace=True)\n",
    "        print('Size of lot_URLs after eliminating duplicates:', len(lot_URLs_dataframe))\n",
    "        print(old_size - len(lot_URLs_dataframe), 'duplicate urls from lot_URLs were removed.')\n",
    "\n",
    "        # save to file self.lot_URLs_path\n",
    "        with open(self.lot_URLs_path, 'w') as fp:\n",
    "            json.dump(lot_URLs_dataframe.to_dict('records'), fp, indent=4, separators=(',', ': '))\n",
    "        print('saved list without duplicates to', self.lot_URLs_path)\n",
    "\n",
    "        \n",
    "    def get_lot_HTML(self):\n",
    "        '''Iterate through the lot_URLs list, fetch the HTML code for each entry and store it as a local file.\n",
    "        We''re doing this to minimize the requests to an auction house website. We're fetching all data and\n",
    "        parse the files later'''\n",
    "\n",
    "        # open lot URLs file\n",
    "        with open(self.lot_URLs_path) as fp:\n",
    "            self.lot_URLs = json.load(fp)\n",
    "        print('Loaded', len(self.lot_URLs),'lots from file', self.lot_URLs_path)    \n",
    "\n",
    "        # iterate through the list\n",
    "        counter = 0\n",
    "        for lot_URLs_entry in self.lot_URLs:\n",
    "                if not lot_URLs_entry['scraped']:\n",
    "                    # get the html and store in file\n",
    "                    self.save_single_lot_HTML_to_local_file(lot_URLs_entry)\n",
    "                    counter += 1\n",
    "                    lot_URLs_entry['scraped'] = True \n",
    "                    # time out to prevent to be blocked\n",
    "                    sleep(randint(61,67))\n",
    "        print('Loaded', counter,'lots and saved to', self.lots_path)\n",
    "\n",
    "        # saving artlytic.lot_URLs list to artlytic.lot_URLs_path file\n",
    "        with open(self.lot_URLs_path, 'w') as fp:\n",
    "            json.dump(self.lot_URLs, fp, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "            print('Saved', len(self.lot_URLs),'lots to file', self.lot_URLs_path) \n",
    "            \n",
    "    \n",
    "    def save_single_lot_HTML_to_local_file(self, lot_URLs_entry):\n",
    "        '''Takes a lot_URLs list entry as input, requests the HTML code and stores it as a local .html file.'''\n",
    "\n",
    "        # Build the path for the local HTML\n",
    "        local_lot_HTML_path = self.data_path + \\\n",
    "                              lot_URLs_entry['auction_house'].lower().replace(\"'\", \"\") + \"/\" + \\\n",
    "                              lot_URLs_entry['timestamp'].replace(':','-').replace('.','-') + \\\n",
    "                              '.html'\n",
    "\n",
    "        # get the HTML code\n",
    "        try:\n",
    "            response = requests.get(lot_URLs_entry['url'])\n",
    "            response.raise_for_status() # ensure we notice bad responses\n",
    "        except (requests.HTTPError, requests.ConnectionError):\n",
    "            print(\"*** HTTPError or ConnectionError while accessing\", lot_URLs_entry['url'])\n",
    "\n",
    "        # store as local file\n",
    "        with open(local_lot_HTML_path, 'w') as fp:\n",
    "            fp.write(response.text)\n",
    "            print(local_lot_HTML_path, \"saved\")\n",
    "            \n",
    "            \n",
    "    def parse_lot_HTMLs(self):\n",
    "        \n",
    "        # populate lists with file data\n",
    "        with open(self.lots_path) as fp:\n",
    "            self.lots = json.load(fp)\n",
    "\n",
    "        with open(self.lot_URLs_path) as fp:\n",
    "            self.lot_URLs = json.load(fp)\n",
    "\n",
    "        # iterate lot_URLs\n",
    "        counter_added_lots = 0\n",
    "        counter_skipped_lots = 0\n",
    "        for lot_URLs_entry in self.lot_URLs:\n",
    "            # if entry has not been parsed yet, parse it\n",
    "            if lot_URLs_entry['parsed'] == False:\n",
    "                # build path of local HTML file\n",
    "                local_HTML_path = self.data_path + \\\n",
    "                                  lot_URLs_entry['auction_house'].lower().replace(\"'\", \"\") + '/' + \\\n",
    "                                  lot_URLs_entry['timestamp'] + '.html' \n",
    "                # needs refactoring!!!!!\n",
    "                lot = dict()\n",
    "                try:\n",
    "                    with open(local_HTML_path, 'rb') as fp:\n",
    "                        html = fp.read()\n",
    "                        if lot_URLs_entry['auction_house']   == \"Christie's\":\n",
    "                            lot = christies.parse(html)\n",
    "                        elif lot_URLs_entry['auction_house'] == \"Sotheby's\":\n",
    "                            lot = sothebys.parse(html)\n",
    "                        elif lot_URLs_entry['auction_house'] == \"Phillips\":\n",
    "                            lot = phillips.parse(html)\n",
    "                        else:\n",
    "                            print('*** Auctionhouse', lot_URLs_entry['auction_house'],'in',lot_URLs_entry['timestamp'],'not found')\n",
    "\n",
    "                        lot['id'] = lot_URLs_entry['timestamp']\n",
    "                        artlytic.lots.append(lot)\n",
    "                        lot_URLs_entry['parsed'] = True\n",
    "                        counter_added_lots += 1\n",
    "                        print(counter_added_lots, \":\", lot_URLs_entry['timestamp'], \"parsed and added to artlytic.lots\")\n",
    "                        \n",
    "\n",
    "                except FileNotFoundError:  \n",
    "                    print('*** could not find file:', local_HTML_path)\n",
    "                    \n",
    "            else: # entry was already parsed\n",
    "                counter_skipped_lots += 1\n",
    "\n",
    "        # write lists to file\n",
    "        \n",
    "        with open(self.lot_URLs_path, 'w') as fp:\n",
    "            json.dump(self.lot_URLs, fp, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "            print('updated parsed status of',counter_added_lots, 'lot_URLs eintries to True and saved to file', self.lot_URLs_path)\n",
    "            print('skipped', counter_skipped_lots, 'lot_URLs entries because they were already parsed.' )\n",
    "            \n",
    "        with open(self.lots_path, 'w') as fp:\n",
    "            json.dump(self.lots, fp, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "            print('added',counter_added_lots, 'new lots to artlytic.lots. New list size:', len(self.lots))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data/search_urls.json to search_URLs with 3427 entries\n",
      "loaded data/lot_urls.json to self.lot_URLs_path 160065 entries\n",
      "loaded data/lots.json to self.lots_path with 94364 entries\n",
      "loaded data/wikiart/wikiart_artists_19th_century.json to self.artists with 857 entries\n"
     ]
    }
   ],
   "source": [
    "artlytic  = Artlytic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning with adding new search URLs. Current list size: 2570\n",
      "Added to search_URLs 857 new entries. New size: 3427\n"
     ]
    }
   ],
   "source": [
    "sothebys.build_search_URLs(artlytic.artists)\n",
    "with open(artlytic.search_URLs_path, 'w') as fp:\n",
    "    json.dump(artlytic.search_URLs, fp, sort_keys=True, indent=4, separators=(',', ': '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of lot_URLs before eliminating duplicates: 166476\n",
      "Size of lot_URLs after eliminating duplicates: 160065\n",
      "6411 duplicate urls from lot_URLs were removed.\n",
      "saved list without duplicates to data/lot_urls.json\n"
     ]
    }
   ],
   "source": [
    "artlytic.eliminate_duplicates_from_lot_URLs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated data/lot_urls.json . File contains now 126740 entries\n"
     ]
    }
   ],
   "source": [
    "with open(artlytic.lot_URLs_path, 'w') as fp:\n",
    "    json.dump(artlytic.lot_URLs, fp, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "print('Updated', artlytic.lot_URLs_path, '. File contains now', len(artlytic.lot_URLs), 'entries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated scraped status in data/search_urls.json\n"
     ]
    }
   ],
   "source": [
    "with open(artlytic.search_URLs_path, 'w') as fp:\n",
    "    json.dump(artlytic.search_URLs, fp, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "print('Updated scraped status in', artlytic.search_URLs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
